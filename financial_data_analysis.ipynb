# Financial Training Data Forensic Analysis
This notebook demonstrates techniques for analyzing financial dataset 
integrity, bias, and quality for AI forensics investigations.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import hashlib
import json
from common.utils import document_analysis, create_lineage_graph
from common.metrics import calculate_bias_metrics, detect_outliers

# 1. Load the dataset with provenance tracking
def load_dataset_with_provenance(file_path):
    """Load dataset while recording provenance information"""
    data = pd.read_csv(file_path)
    
    # Generate and store dataset hash
    with open(file_path, 'rb') as f:
        data_hash = hashlib.sha256(f.read()).hexdigest()
    
    # Record metadata
    provenance = {
        "filename": file_path,
        "hash": data_hash,
        "timestamp": datetime.now().isoformat(),
        "shape": data.shape,
        "columns": list(data.columns)
    }
    
    return data, provenance

# 2. Demographic representation analysis
def analyze_demographic_representation(data, demographic_cols):
    """Analyze demographic representation and balance in financial data"""
    report = {}
    
    for col in demographic_cols:
        # Calculate distribution
        distribution = data[col].value_counts(normalize=True)
        
        # Calculate representation metrics
        report[col] = {
            "distribution": distribution.to_dict(),
            "entropy": stats.entropy(distribution),
            "minority_class_size": distribution.min(),
            "majority_class_size": distribution.max(),
            "imbalance_ratio": distribution.max() / distribution.min() if distribution.min() > 0 else float('inf')
        }
        
        # Create visualization
        plt.figure(figsize=(10, 6))
        sns.countplot(data=data, x=col)
        plt.title(f"Distribution of {col}")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"demographic_distribution_{col}.png")
        
    return report

# 3. Financial attribute bias analysis
def analyze_financial_biases(data, sensitive_cols, financial_cols):
    """Analyze potential biases in financial attributes across demographic groups"""
    bias_report = {}
    
    for sens_col in sensitive_cols:
        bias_report[sens_col] = {}
        
        for fin_col in financial_cols:
            # Group statistics
            group_stats = data.groupby(sens_col)[fin_col].agg(['mean', 'median', 'std']).reset_index()
            
            # Statistical tests for significance
            groups = data[sens_col].unique()
            p_values = []
            
            # Perform pairwise statistical tests
            for i in range(len(groups)):
                for j in range(i+1, len(groups)):
                    group1 = data[data[sens_col] == groups[i]][fin_col]
                    group2 = data[data[sens_col] == groups[j]][fin_col]
                    stat, p = stats.ttest_ind(group1, group2, equal_var=False)
                    p_values.append({"groups": (groups[i], groups[j]), "p_value": p})
            
            # Record results
            bias_report[sens_col][fin_col] = {
                "group_statistics": group_stats.to_dict(),
                "statistical_tests": p_values,
                "max_disparity_ratio": group_stats['mean'].max() / group_stats['mean'].min() if group_stats['mean'].min() > 0 else float('inf')
            }
            
            # Create visualization
            plt.figure(figsize=(12, 6))
            sns.boxplot(x=sens_col, y=fin_col, data=data)
            plt.title(f"Distribution of {fin_col} by {sens_col}")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(f"financial_bias_{sens_col}_{fin_col}.png")
    
    return bias_report

# 4. Data quality assessment
def assess_data_quality(data):
    """Assess overall data quality for forensic analysis"""
    quality_report = {
        "completeness": {},
        "consistency": {},
        "outliers": {},
        "statistical_properties": {}
    }
    
    # Completeness analysis
    quality_report["completeness"]["missing_values"] = data.isnull().sum().to_dict()
    quality_report["completeness"]["missing_percent"] = (data.isnull().sum() / len(data) * 100).to_dict()
    
    # Consistency analysis
    # ... implementation for detecting inconsistencies in financial data
    
    # Outlier detection
    for col in data.select_dtypes(include=[np.number]).columns:
        outliers = detect_outliers(data[col])
        quality_report["outliers"][col] = {
            "count": len(outliers),
            "percent": len(outliers) / len(data) * 100,
            "indices": outliers.tolist() if len(outliers) < 100 else "too many to list"
        }
    
    # Generate visualizations for key quality issues
    # ... implementation for quality visualization
    
    return quality_report

# 5. Source verification
def verify_data_sources(data, metadata, documentation):
    """Verify data sources against provided documentation"""
    verification_report = {
        "source_match": True,
        "discrepancies": [],
        "warnings": []
    }
    
    # Example verification checks
    # ... implementation for source verification
    
    return verification_report

# Main analysis workflow
def main():
    # Load dataset
    financial_data, provenance = load_dataset_with_provenance("datasets/synthetic/financial_training_data.csv")
    
    # Load documentation
    with open("datasets/synthetic/financial_data_documentation.json", "r") as f:
        documentation = json.load(f)
    
    # Run analyses
    demographic_report = analyze_demographic_representation(
        financial_data, 
        demographic_cols=["gender", "age_group", "location"]
    )
    
    bias_report = analyze_financial_biases(
        financial_data,
        sensitive_cols=["gender", "age_group", "location"],
        financial_cols=["income", "credit_score", "loan_amount", "interest_rate"]
    )
    
    quality_report = assess_data_quality(financial_data)
    
    source_verification = verify_data_sources(
        financial_data,
        provenance,
        documentation
    )
    
    # Compile comprehensive report
    forensic_report = {
        "provenance": provenance,
        "demographic_representation": demographic_report,
        "bias_analysis": bias_report,
        "data_quality": quality_report,
        "source_verification": source_verification,
        "timestamp": datetime.now().isoformat(),
        "analysis_version": "1.0.0"
    }
    
    # Save report
    with open("financial_data_forensic_report.json", "w") as f:
        json.dump(forensic_report, f, indent=2)
    
    # Create visual summary
    # ... implementation for report visualization

if __name__ == "__main__":
    main()